# Awesome-AI-Scientists [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A curated list of awesome resources on AI Scientists, based on our survey "A Comprehensive Survey of AI Scientists".

<!-- ![Loading roadmap](asset/roadmap.png "outline") -->

<!--
## Newsüî•

* **[2024/11/21]** Initial release of our survey is available on [arXiv](https://arxiv.org/abs/2411.14499v1).
* **[2025/07/18]** Initial release of the Awesome-AI-Scientists GitHub repository.
-->

<!--
## Contactüìë
If you have any suggestions or find our work helpful, feel free to contact us  
Email: fenglixu@tsinghua.edu.cn
-->

<!--
If this list helps your research, please ‚≠ê and cite:

```bibtex
@article{ding2025worldmodels,
  title={Understanding World or Predicting Future? A Comprehensive Survey of World Models},
  author={Ding, Jingtao and Zhang, Yunke and Shang, Yu and Zhang, Yuheng and Zong, Zefang and Feng, Jie and Yuan, Yuan and Su, Hongyu and Li, Nian and Sukiennik, Nicholas and Xu, Fengli and Li, Yong},
  journal={ACM Computing Surveys},
  year={2025}
}
```
-->

## Table of Contents üçÉ
- [Awesome-AI-Scientists ](#awesome-ai-scientists-)
  - [Table of Contents üçÉ](#table-of-contents-)
  - [Functional Roles of AI Scientists](#functional-roles-of-ai-scientists)
    - [Deep Research](#deep-research)
    - [Ideation](#ideation)
    - [Autonomous Experiments](#autonomous-experiments)
    - [Scientific Presentation](#scientific-presentation)
    - [Research Evaluation](#research-evaluation)
  - [Human-AI Scientist Collaboration](#human-ai-scientist-collaboration)
    - [Human-in-the-Loop](#human-in-the-loop)
    - [Human-out-of-the-Loop](#human-out-of-the-loop)
  - [AI Scientist by Research Domain](#ai-scientist-by-research-domain)
    - [Physical Science](#physical-science)
    - [Life Science](#life-science)
    - [Social Science](#social-science)
    - [Data Science](#data-science)
  - [Broader Impacts of AI Scientists](#broader-impacts-of-ai-scientists)

## Functional Roles of AI Scientists

### Deep Research

| Title                                                        | Pub. & Date     | Code / Project URL                                           |
| :----------------------------------------------------------- | :-------------- | :----------------------------------------------------------- |
| **[Tongyi DeepResearch: A New Era of Open-Source AI Researchers](https://arxiv.org/abs/2510.24701)** | ArXiv 2025      | [![Star](https://img.shields.io/github/stars/Alibaba-NLP/DeepResearch.svg?style=social&label=Star)](https://github.com/Alibaba-NLP/DeepResearch) |
| **[Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning](https://arxiv.org/abs/2503.09516)** | ArXiv 2025 | [![Star](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1.svg?style=social&label=Star)](https://github.com/PeterGriffinJin/Search-R1) |
| **[WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)** | ArXiv 2025      | [![Star](https://img.shields.io/github/stars/RUC-NLPIR/WebThinker.svg?style=social&label=Star)](https://github.com/RUC-NLPIR/WebThinker) |
| **[WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)** | ArXiv 2025      | [![Star](https://img.shields.io/github/stars/Alibaba-NLP/DeepResearch.svg?style=social&label=Star)](https://github.com/Alibaba-NLP/DeepResearch) |
| **[AgentRxiv: Towards Collaborative Autonomous Research](https://arxiv.org/abs/2503.18102)** | ArXiv 2025      | [![Website](https://img.shields.io/badge/Project-Page-blue)](https://agentrxiv.github.io/) |
| **[AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning](https://arxiv.org/abs/2406.11200)** | NeurIPS 2024    | [![Star](https://img.shields.io/github/stars/zou-group/avatar.svg?style=social&label=Star)](https://github.com/zou-group/avatar) |
| **[DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents](https://arxiv.org/abs/2506.11763)** | ArXiv 2025      | [![Star](https://img.shields.io/github/stars/Ayanami0730/deep_research_bench.svg?style=social&label=Star)](https://github.com/Ayanami0730/deep_research_bench) |
| **[Open Deep Research (LangChain)](https://github.com/langchain-ai/open_deep_research)** | GitHub 2025     | [![Star](https://img.shields.io/github/stars/langchain-ai/open_deep_research.svg?style=social&label=Star)](https://github.com/langchain-ai/open_deep_research) |
| **[Introducing Deep Research (OpenAI)](https://openai.com/index/introducing-deep-research/)** | OpenAI 2025     | [Product Page](https://openai.com/index/introducing-deep-research/) |
| **[Sonar Deep Research (Perplexity)](https://docs.perplexity.ai/getting-started/models/models/sonar-deep-research)** | Perplexity 2025 | [Model Card](https://docs.perplexity.ai/getting-started/models/models/sonar-deep-research) |
| **[Qwen Deep Research](https://www.alibabacloud.com/help/en/model-studio/qwen-deep-research)** | Qwen AI 2025    | [Documentation](https://www.alibabacloud.com/help/en/model-studio/qwen-deep-research) |

### Ideation
| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |

### Autonomous Experiments


| Title | Category | Publication & Year | Code / Project URL |
|---|---|---|---|
| [Code2MCP: Transforming Code Repositories into MCP Services](https://arxiv.org/abs/2509.05941) | Compiler | arXiv 2025 | [Code](https://github.com/DEFENSE-SEU/Code2MCP) |
| [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents](https://arxiv.org/abs/2509.06917) | Compiler | arXiv 2025 | [Code](https://github.com/jmiao24/Paper2Agent) |
| [EnvX: Agentize Everything with Agentic AI](https://arxiv.org/abs/2509.08088) | Compiler | arXiv 2025 | - |
| [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192) | Compiler | arXiv 2025 | [Code](https://github.com/going-doer/Paper2Code) |
| [Mathematical discoveries from program search with large language models](https://www.nature.com/articles/s41586-023-06924-6) | Compiler | Nature 2024 | [Code](https://github.com/google-deepmind/funsearch) |
| [AlphaEvolve: A coding agent for scientific and algorithmic discovery](https://arxiv.org/abs/2506.13131) | Compiler | arXiv 2025 | [Blog](https://deepmind.google/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) |
| [X-evolve: Solution space evolution powered by large language models](https://arxiv.org/abs/2508.07932) | Compiler | arXiv 2025 | [Code](https://github.com/zhaiyi000/x-evolve) |
| [ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution](https://arxiv.org/abs/2509.19349) | Compiler | arXiv 2025 | [Code](https://github.com/SakanaAI/ShinkaEvolve) |
| [DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively](https://arxiv.org/abs/2509.26603) | Compiler | arXiv 2025 | [Code](https://github.com/ResearAI/DeepScientist) |
| [OpenFOAMGPT 2.0: end-to-end, trustworthy automation for computational fluid dynamics](https://arxiv.org/abs/2504.19338) | Simulator | arXiv 2025 | - |
| [ChemCrow: Augmenting large-language models with chemistry tools](https://www.nature.com/articles/s42256-024-00832-8) | Simulator | Nat. Mach. Intell. 2024 | [Code](https://github.com/ur-whitelab/chemcrow-public) |
| [A mobile robotic chemist](https://www.nature.com/articles/s41586-020-2442-2) | Robot | Nature 2020 | [BitBucket](https://bitbucket.org/ben_burger/kuka_workflow) |
| [An autonomous laboratory for the accelerated synthesis of novel materials](https://www.nature.com/articles/s41586-023-06734-w) | Robot | Nature 2023 | - |
| [Autonomous chemical research with large language models](https://www.nature.com/articles/s41586-023-06792-0) | Robot | Nature 2023 | [Code](https://github.com/gomesgroup/coscientist) |
| [A multimodal robotic platform for multi-element electrocatalyst discovery](https://www.nature.com/articles/s41586-025-09640-5) | Robot | Nature 2025 | [Code](https://github.com/zhang21mit/CRESt) |
| [Biomni: A general-purpose biomedical AI agent](https://doi.org/10.1101/2025.05.30.656746 ) | human | biorXiv2025 | [![Website](https://img.shields.io/badge/Project-Page-blue)](https://biomni.stanford.edu/) |
| [CRISPR-GPT for agentic automation of gene-editing experiments](https://doi.org/10.1038/s41551-025-01463-z) | human | Nature Biomedical Engineering 2025 | - |
| [Towards an AI co-scientist](https://arxiv.org/abs/2502.18864) | human | arXiv 2025 | - |
| [Robin: A multi-agent system for automating scientific discovery](https://arxiv.org/abs/2505.13400) | human | arXiv 2025 | [Code](https://github.com/Future-House/robin/?tab=readme-ov-file) |
| [What is the Model Conetext Protocol(MCP)?](https://modelcontextprotocol.io/docs/getting-started/intro) | Platform & Protocol | 2025 | [GitHub](https://github.com/modelcontextprotocol) |
| [Democratizing AI scientists using ToolUniverse](https://arxiv.org/abs/2509.23426) | Platform & Protocol | arXiv 2025 | [![Website](https://img.shields.io/badge/Project-Page-blue)](https://aiscientist.tools/) |
| [Scientific Intelligence Context Protocol(SCP)](https://github.com/open-sciencelab/scp) | Platform & Protocol | 2025 | [Code](https://github.com/open-sciencelab/scp) |

| ------------------------------------------------------------ | ------------------- | ---- | ------------------------------------------------------------ |
| ------------------------------------------------------------ | ------------------- | ---- | ------------------------------------------------------------ |
| ------------------------------------------------------------ | ------------------- | ---- |------------------------------------------------------------ |

### Scientific Presentation
| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |
| [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery  **(AI Scientist)**](https://arxiv.org/abs/2408.06292) | arxiv 2024 | [![Star](https://img.shields.io/github/stars/SakanaAI/AI-Scientist.svg?style=social&label=Star)](https://github.com/SakanaAI/AI-Scientist) |
| [The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search  **(AI Scientist-v2)**](https://arxiv.org/abs/2504.08066) | arxiv 2025 | [![Star](https://img.shields.io/github/stars/SakanaAI/AI-Scientist-v2.svg?style=social&label=Star)](https://github.com/SakanaAI/AI-Scientist-v2) |
| [AI-Researcher: Autonomous Scientific Innovation  **(AI-Researcher)**](https://arxiv.org/abs/2505.18705) | arxiv 2025 | [![Star](https://img.shields.io/github/stars/HKUDS/AI-Researcher.svg?style=social&label=Star)](https://github.com/HKUDS/AI-Researcher) |
| [Agent Laboratory: Using LLM Agents as Research Assistants  **(Agent Laboratory)**](https://arxiv.org/abs/2501.04227) | arxiv 2025 | [![Star](https://img.shields.io/github/stars/SamuelSchmidgall/AgentLaboratory.svg?style=social&label=Star)](https://github.com/SamuelSchmidgall/AgentLaboratory) |
| [AgentRxiv: Towards Collaborative Autonomous Research  **(AgentRxiv)**](https://arxiv.org/abs/2503.18102) | arxiv 2025 | [![Website](https://img.shields.io/badge/Website-9cf)](https://agentrxiv.github.io/) |
| [Autonomous LLM-Driven Research ‚Äî from Data to Human-Verifiable Research Papers **(data-to-paper)**](https://arxiv.org/abs/2404.17605) | NEJM AI 2025 | [![Star](https://img.shields.io/github/stars/Technion-Kishony-lab/data-to-paper.svg?style=social&label=Star)](https://github.com/Technion-Kishony-lab/data-to-paper) |
| [Zochi technical report  **(Zochi)**](https://github.com/IntologyAI/Zochi/blob/main/Zochi_Technical_Report.pdf) | 2025 | [![Star](https://img.shields.io/github/stars/IntologyAI/Zochi.svg?style=social&label=Star)](https://github.com/IntologyAI/Zochi) |
| [CycleResearcher: Improving Automated Research via Automated Review  **(CycleResearcher)**](https://arxiv.org/abs/2411.00816) | ICLR 2025 | [![Star](https://img.shields.io/github/stars/zhu-minjun/Researcher.svg?style=social&label=Star)](https://github.com/zhu-minjun/Researcher) |
| [AutoSurvey: Large Language Models Can Automatically Write Surveys  **(AutoSurvey)**](https://arxiv.org/abs/2406.10252) | NeurIPS 2024 | [![Star](https://img.shields.io/github/stars/AutoSurveys/AutoSurvey.svg?style=social&label=Star)](https://github.com/AutoSurveys/AutoSurvey) |
| [SURVEYFORGE: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing  **(SURVEYFORGE)**](https://aclanthology.org/2025.acl-long.609/) | ACL 2025 | [![Star](https://img.shields.io/github/stars/Alpha-Innovator/SurveyForge.svg?style=social&label=Star)](https://github.com/Alpha-Innovator/SurveyForge) |
| [SurveyX: Academic Survey Automation via Large Language Models  **(SurveyX)**](https://arxiv.org/abs/2502.14776) | arxiv 2025 | [![Star](https://img.shields.io/github/stars/IAAR-Shanghai/SurveyX.svg?style=social&label=Star)](https://github.com/IAAR-Shanghai/SurveyX) |
| [Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers  **(Paper2Poster)**](https://arxiv.org/abs/2505.21497) | NeurIPS 2025 | [![Star](https://img.shields.io/github/stars/Paper2Poster/Paper2Poster.svg?style=social&label=Star)](https://github.com/Paper2Poster/Paper2Poster) |
| [PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides  **(PPTAgent)**](https://aclanthology.org/2025.emnlp-main.728/) | EMNLP 2025 | [![Star](https://img.shields.io/github/stars/icip-cas/PPTAgent.svg?style=social&label=Star)](https://github.com/icip-cas/PPTAgent) |
| [PresentAgent: Multimodal Agent for Presentation Video Generation  **(PresentAgent)**](https://aclanthology.org/2025.emnlp-demos.58/) | EMNLP 2025 | [![Star](https://img.shields.io/github/stars/AIGeeksGroup/PresentAgent.svg?style=social&label=Star)](https://github.com/AIGeeksGroup/PresentAgent) |
| [Paper2Video: Automatic Video Generation from Scientific Papers  **(Paper2Video)**](https://arxiv.org/abs/2510.05096) | SEA @ NeurIPS 2025 | [![Star](https://img.shields.io/github/stars/showlab/Paper2Video.svg?style=social&label=Star)](https://github.com/showlab/Paper2Video) |
| [Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents  **(Paper2Agent)**](https://arxiv.org/abs/2509.06917) | arxiv 2025 | [![Star](https://img.shields.io/github/stars/jmiao24/Paper2Agent.svg?style=social&label=Star)](https://github.com/jmiao24/Paper2Agent) |

### Research Evaluation
| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |
| [Automated Peer Reviewing in Paper SEA: Standardization, Evaluation, and Analysis  **(SEA)**](https://arxiv.org/abs/2407.12857) | EMNLP 2025 | [![Star](https://img.shields.io/github/stars/ecnu-sea/SEA.svg?style=social&label=Star)](https://github.com/ecnu-sea/sea) |
| [Deepreview: Improving llm-based paper review with human-like deep thinking process  **(Deepreview)**](https://arxiv.org/abs/2503.08569) | ACL 2025 | [![Star](https://img.shields.io/github/stars/zhu-minjun/Researcher.svg?style=social&label=Star)](https://github.com/zhu-minjun/Researcher) |
| [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review  **(Treereview)**](https://arxiv.org/abs/2506.07642) | EMNLP 2025 | [![Star](https://img.shields.io/github/stars/YuanChang98/tree-review.svg?style=social&label=Star)](https://github.com/YuanChang98/tree-review) |
| [GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation  **(GraphEval)**](https://arxiv.org/abs/2503.12600) | ICLR 2025 | [![Star](https://img.shields.io/github/stars/ulab-uiuc/GraphEval.svg?style=social&label=Star)](https://github.com/ulab-uiuc/GraphEval) |
| [CycleResearcher: Improving Automated Research via Automated Review  **(CycleResearcher)**](https://arxiv.org/abs/2411.00816) | ICLR 2025 | [![Star](https://img.shields.io/github/stars/zhu-minjun/Researcher.svg?style=social&label=Star)](https://github.com/zhu-minjun/Researcher) |
| [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery  **(AI Scientist)**](https://arxiv.org/abs/2408.06292) | arxiv 2024 | [![Star](https://img.shields.io/github/stars/SakanaAI/AI-Scientist.svg?style=social&label=Star)](https://github.com/SakanaAI/AI-Scientist) |
| [The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search  **(AI Scientist-v2)**](https://arxiv.org/abs/2504.08066) | arxiv 2025 | [![Star](https://img.shields.io/github/stars/SakanaAI/AI-Scientist-v2.svg?style=social&label=Star)](https://github.com/SakanaAI/AI-Scientist-v2) |

## Human-AI Scientist Collaboration

### Human-in-the-Loop
| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |

### Human-out-of-the-Loop
| Title                                                        | Pub.&Date           | Code/Project URL                                             | 
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |
| [The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery  **(AI Scientist)**](https://arxiv.org/abs/2408.06292) | arxiv 2024 | [![Star](https://img.shields.io/github/stars/SakanaAI/AI-Scientist.svg?style=social&label=Star)](https://github.com/SakanaAI/AI-Scientist) |
| [The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search  **(AI Scientist-v2)**](https://arxiv.org/abs/2504.08066) | arxiv 2025 | [![Star](https://img.shields.io/github/stars/SakanaAI/AI-Scientist-v2.svg?style=social&label=Star)](https://github.com/SakanaAI/AI-Scientist-v2) |
| [Zochi technical report  **(Zochi)**](https://github.com/IntologyAI/Zochi/blob/main/Zochi_Technical_Report.pdf) | 2025 | [![Star](https://img.shields.io/github/stars/IntologyAI/Zochi.svg?style=social&label=Star)](https://github.com/IntologyAI/Zochi) |
## AI Scientist by Research Domain

### Physical Science
| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |
| [Augmenting large language models with chemistry tools](<https://www.nature.com/articles/s42256-024-00832-8>) | Nature Machine Intelligence, 2024 |None  |
| [Autonomous chemical research with large language models.](<https://www.nature.com/articles/s41586-023-06792-0>) | Nature, 2023 | [![Star](https://img.shields.io/github/stars/gomesgroup/coscientist.svg?style=social&label=Star)](https://github.com/gomesgroup/coscientist) |
| [The Virtual Lab of AI Agents Designs New SARS-CoV-2 Nanobodies **(VirtualLab)**](<https://www.nature.com/articles/s41586-025-09442-9>) | Nature, 2025 | [![Star](https://img.shields.io/github/stars/zou-group/virtual-lab.svg?style=social&label=Star)](https://github.com/zou-group/virtual-lab) |
| [A Bayesian machine scientist to aid in the solution of challenging scientific problems](<https://www.science.org/doi/full/10.1126/sciadv.aav6971>) | Science advances, 2020 | None |
| [AI Feynman: A physics-inspired method for symbolic regression **(AI Feynman)**](<https://www.science.org/doi/full/10.1126/sciadv.aay2631>) | Science advances, 2020 | [![Star](https://img.shields.io/github/stars/SJ001/AI-Feynman.svg?style=social&label=Star)](https://github.com/SJ001/AI-Feynman) |
| [Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical law](<https://iopscience.iop.org/article/10.3847/1538-4357/ad014c/meta>) | The Astrophysical, 2023 | None |
| [A neural symbolic model for space physics](<https://www.nature.com/articles/s42256-025-01126-3>) | Nature Machine Intelligence, 2025 | [![Star](https://img.shields.io/github/stars/Jie0618/PhysicsRegression.svg?style=social&label=Star)](https://github.com/Jie0618/PhysicsRegression) |
| [MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses **(MOOSE-Chem)**](<https://openreview.net/forum?id=X9OfMNNepI>) | ICLR, 2025 | [![Star](https://img.shields.io/github/stars/ZonglinY/MOOSE-Chem.svg?style=social&label=Star)](https://github.com/ZonglinY/MOOSE-Chem) |
| [Beyond designer‚Äôs knowledge: Generating materials design hypotheses via a large language model](<https://www.sciencedirect.com/science/article/pii/S1359645425005932>) | Acta Materialia, 2025 | None |
| [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search](<https://arxiv.org/pdf/2505.19209>) | arXiv:2505.19209, 2025 | [![Star](https://img.shields.io/github/stars/ZonglinY/MOOSE-Chem2.svg?style=social&label=Star)](https://github.com/ZonglinY/MOOSE-Chem2) |
| [AI-Newton: A Concept-Driven Physical Law Discovery System without Prior Physical Knowledge](<https://arxiv.org/pdf/2504.01538?>) | Nature News(<https://www.nature.com/articles/d41586-025-03659-4>), 2025 |[![Star](https://img.shields.io/github/stars/Science-Discovery/AI-Newton.svg?style=social&label=Star)](https://github.com/Science-Discovery/AI-Newton) |
| [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](<https://arxiv.org/pdf/2505.17873>) | arXiv:2505.17873, 2025 | [![Star](https://img.shields.io/github/stars/wanhaoliu/MOOSE-Chem3.svg?style=social&label=Star)](https://github.com/wanhaoliu/MOOSE-Chem3) |
| [EarthLink: A Self-Evolving AI Agent for Climate Science](<https://arxiv.org/pdf/2507.17311?>) | arXiv:2507.17311?, 2025 | [Homepage](https://earthlink.intern-ai.org.cn) |
| [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](<https://arxiv.org/pdf/2505.17139>) | arXiv:2505.17139, 2025 | [Huggingface Page](https://huggingface.co/ai-earth) |




### Life Science
| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |
| [BioInformatics Agent (BIA): Unleashing the Power of Large Language Models to Reshape Bioinformatics Workflow **(BIA)**](<https://www.biorxiv.org/content/10.1101/2024.05.22.595240v2.abstract>) | Bioinformatics, 2025 |[![Star](https://img.shields.io/github/stars/biagent-dev/bia.svg?style=social&label=Star)](https://github.com/biagent-dev/bia)  |
| [Cellagent: An LLM-Driven Multi-Agent Framework for Automated Single-Cell Data Analysis **(CellAgent)**](<https://arxiv.org/abs/2407.09811>) | arXiv:2407.09811, 2024 | [Homepage](http://cell.agent4science.cn/) |
| [CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments **(CRISPR-GPT)**](<https://arxiv.org/html/2404.18021v1>) | arXiv:2404.18021, 2024 | None |
| [PerTurboAgent: A Self-Planning Agent for Boosting Sequential Perturb-Seq Experiments **(PerTurboAgent)**](<https://www.biorxiv.org/content/10.1101/2025.05.25.656020v1>) | bioRxiv, 2025 | None|
| [BiodiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments **(BiodiscoveryAgent)**](<https://arxiv.org/abs/2405.17631>) | arXiv:2405.17631, 2024 | [![Star](https://img.shields.io/github/stars/snap-stanford/BioDiscoveryAgent.svg?style=social&label=Star)](https://github.com/snap-stanford/BioDiscoveryAgent) |
| [AI Mirrors Experimental Science to Uncover a Mechanism of Gene Transfer Crucial to Bacterial Evolution ](<https://www.sciencedirect.com/science/article/pii/S0092867425009730>) | Cell, 2025 | None |
| [Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment **(LLM-Hypothesis)**](<https://arxiv.org/abs/2405.12258>) | Journal of the Royal Society Interface, 2025 | None |
| [SpatialAgent: An Autonomous AI Agent for Spatial Biology **(SpatialAgent)**](<https://www.biorxiv.org/content/10.1101/2025.04.03.646459v1>) | bioRxiv, 2025 | [![Star](https://img.shields.io/github/stars/Genentech/SpatialAgent.svg?style=social&label=Star)](https://github.com/Genentech/SpatialAgent) |
| [BioMaster: Multi-Agent System for Automated Bioinformatics Analysis Workflow **(BioMaster)**](<https://www.biorxiv.org/content/10.1101/2025.01.23.634608v1>) | bioRxiv, 2025 | [![Star](https://img.shields.io/github/stars/ai4nucleome/BioMaster.svg?style=social&label=Star)](https://github.com/ai4nucleome/BioMaster/tree/master) |
| [Biomni: A General-Purpose Biomedical AI Agent Integrating 25 Domains to Generate Hypotheses **(Biomni)**](<https://biomni.stanford.edu/paper.pdf>) | bioRxiv, 2025 | [![Star](https://img.shields.io/github/stars/snap-stanford/biomni.svg?style=social&label=Star)](https://github.com/snap-stanford/biomni) |
| [STELLA: Self-Evolving LLM Agent for Biomedical Research **(STELLA)**](<https://arxiv.org/abs/2507.02004>) | arXiv:2507.02004, 2025 | [![Star](https://img.shields.io/github/stars/zaixizhang/STELLA.svg?style=social&label=Star)](https://github.com/zaixizhang/STELLA) |
| [The Virtual Lab of AI Agents Designs New SARS-CoV-2 Nanobodies **(VirtualLab)**](<https://www.nature.com/articles/s41586-025-09442-9>) | Nature, 2025 | [![Star](https://img.shields.io/github/stars/zou-group/virtual-lab.svg?style=social&label=Star)](https://github.com/zou-group/virtual-lab) |
| [Proteus: Automating Exploratory Proteomics Research via Language Models **(Proteus)**](<https://arxiv.org/abs/2411.03743>) | arXiv:2504.12345, 2025 | None |
### Social Science
| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |

### Data Science
| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |
| [DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning  **(DS-Agent)**](https://dl.acm.org/doi/abs/10.5555/3692070.3692738) | ICML2024               | [![Star](https://img.shields.io/github/stars/guosyjlu/DS-Agent?style=social)](https://github.com/guosyjlu/DS-Agent) |
| [Dagent: A relational database-driven data analysis report generation agent  **(Dagent)**](https://arxiv.org/abs/2503.13269) | Arxiv 2025              | None |
| [Data Interpreter: An LLM Agent For Data Science  **(Data Interpreter)**](https://aclanthology.org/2025.findings-acl.1016/) | ACL 2025ÔºàFindingsÔºâ             | [![Website](https://img.shields.io/badge/Website-9cf?style=social)](https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/interpreter/intro.html) |
| [Exploring LLM agents for cleaning tabular machine learning datasets ](https://arxiv.org/abs/2503.06664) | ICLR 2025 Workshop          | None |
| [AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions  **(AutoKaggle)**](https://orbilu.uni.lu/handle/10993/64877) | ICLR2025               | [![Star](https://img.shields.io/github/stars/multimodal-art-projection/AutoKaggle?style=social)](https://github.com/multimodal-art-projection/AutoKaggle) |
| [AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML  **(AutoML-Agent)**](https://arxiv.org/abs/2410.02958) | ICML2025              | [![Star](https://img.shields.io/github/stars/deepauto-ai/automl-agent?style=social)](https://github.com/deepauto-ai/automl-agent) |
| [LAMBDA: A Large Model Based Data Agent  **(LAMBDA)**](https://www.tandfonline.com/doi/full/10.1080/01621459.2025.2510000#abstract) | Journal of the American Statistical Association 2025             | [![Star](https://img.shields.io/github/stars/AMA-CMFAI/LAMBDA?style=social)](https://github.com/AMA-CMFAI/LAMBDA) |
| [MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue ReSolution  **(MAGIS)**](https://proceedings.neurips.cc/paper_files/paper/2024/hash/5d1f02132ef51602adf07000ca5b6138-Abstract-Conference.html) | NeurIPS2025               | [![Star](https://img.shields.io/github/stars/co-evolve-lab/magis?style=social)](https://github.com/co-evolve-lab/magis) |
| [Budgetmlagent: A cost-effective LLM multi-agent system for automating machine learning tasks  **(Budgetmlagent)**](https://dl.acm.org/doi/full/10.1145/3703412.3703416) | AIMLSystems '24              | None |
| [From data to story: Towards automatic animated data video creation with LLM-based multi-agent systems ](https://ieeexplore.ieee.org/abstract/document/10766492) | 2024 IEEE VIS Workshop           | None |
| [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science  **(DeepAnalyze)**](https://arxiv.org/abs/2510.16872) | Arxiv 2025             | [![Star](https://img.shields.io/github/stars/ruc-datalab/DeepAnalyze)](https://github.com/ruc-datalab/DeepAnalyze) |



<!--
ËØ∑ÂèÇËÄÉ‰∏ãÈù¢ÁöÑÊ†ºÂºèÔºö

## Table of Contents üçÉ

* [Roadmap of world models in deep learning era](#Roadmap-of-world-models-in-deep-learning-era)
    * [Model-based RL](#Model-based-RL)
    * [Self-supervised learning](#Self-supervised-learning)
    * [LLM/MLLM](#LLMMLLM)
    * [Interactive 3D environment](#Interactive-3D-environment)

* [3 Implicit Representation of the External World](#3-implicit-representation-of-the-external-world)
    * [3.1 World Model in Decision Making](#31-World-Model-in-Decision-Making)
    * [3.2 World Knowledge Learned by Models](#32-World-Knowledge-Learned-by-Models)
* [4 Future Prediction of the Physical World](#4-future-predictions-of-the-external-world)
    * [4.1 World Model as Video Generation](#41-World-Model-as-Video-Generation)
    * [4.2 World Model as Embodied Environment](#42-World-Model-as-Embodied-Environment)
* [5 Applications of World Models](#5-applications-of-world-models)
    * [5.1 Game Intelligence](#51-Game-Intelligence)
    * [5.2 Embodied Intelligence](#52-Embodied-Intelligence)
    * [5.3 Urban Intelligence](#53-Urban-Intelligence)
    * [5.4 Societal Intelligence](#54-Societal-Intelligence)

## Roadmap of world models in deep learning era

![Loading roadmap](asset/roadmap.png "outline")

### Model-based RL

| Title                                                        | Pub. & Date  | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------ | ------------------------------------------------------------ |
| [Recurrent world models facilitate policy evolution  **(RWM)**](https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf) | NeurIPS 2018 | [![Website](https://img.shields.io/badge/Website-9cf)](https://worldmodels.github.io/) |
| [Learning Latent Dynamics for Planning from Pixels  **(PlaNet)**](https://arxiv.org/pdf/1811.04551) | ICML 2019    | [![Star](https://img.shields.io/github/stars/google-research/planet.svg?style=social&label=Star)](https://github.com/google-research/planet) |
| [Dream to control: Learning behaviors by latent imagination  **(Dreamer V1)**](https://arxiv.org/pdf/1912.01603) | ICLR 2020    | [![Star](https://img.shields.io/github/stars/danijar/dreamer.svg?style=social&label=Star)](https://github.com/danijar/dreamer) |
| [Mastering atari with discrete world models  **(Dreamer V2)**](https://arxiv.org/pdf/2010.02193) | ICLR 2021    | [![Star](https://img.shields.io/github/stars/danijar/dreamerv2.svg?style=social&label=Star)](https://github.com/danijar/dreamerv2) |
| [Temporal Difference Learning for Model Predictive Control  **(TD-MPC1)**](https://arxiv.org/pdf/2203.04955) | ICML 2023    | [![Star](https://img.shields.io/github/stars/nicklashansen/tdmpc.svg?style=social&label=Star)](https://github.com/nicklashansen/tdmpc) |
| [Mastering Diverse Domains through World Models  **(Dreamer V3)**](https://arxiv.org/pdf/2301.04104) | 2023         | [![Star](https://img.shields.io/github/stars/danijar/dreamerv3.svg?style=social&label=Star)](https://github.com/danijar/dreamerv3) |
| [TD-MPC2: Scalable, Robust World Models for Continuous Control  **(TD-MPC2)**](https://openreview.net/pdf?id=Oxh5CstDJU) | ICLR 2024    | [![Star](https://img.shields.io/github/stars/nicklashansen/tdmpc2.svg?style=social&label=Star)](https://github.com/nicklashansen/tdmpc2) |
| [PWM: Policy Learning with Multi-Task World Models  **(PWM)**](https://openreview.net/pdf?id=Oxh5CstDJU) | ICLR 2025    | [![Star](https://img.shields.io/github/stars/imgeorgiev/PWM.svg?style=social&label=Star)](https://github.com/imgeorgiev/PWM) |



### Self-supervised learning

| Title                                                        | Pub.&Date | Code/Project URL                                             |
| ------------------------------------------------------------ | --------- | ------------------------------------------------------------ |
| [A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27  **(JEPA)**](https://openreview.net/pdf?id=BZ5a1r-kVsf) | 2024      | ‚Äî                                                            |
| [DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot Planning  **(DINO-WM)**](https://arxiv.org/pdf/2411.04983) | 2024      | [![Star](https://img.shields.io/github/stars/gaoyuezhou/dino_wm.svg?style=social&label=Star)](https://github.com/gaoyuezhou/dino_wm) |
| [Revisiting Feature Prediction for Learning Visual Representations from Video  **(V-JEPA)**](https://arxiv.org/pdf/2404.08471) | 2024      | [![Star](https://img.shields.io/github/stars/facebookresearch/jepa.svg?style=social&label=Star)](https://github.com/facebookresearch/jepa) |
| [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning  **(V-JEPA2)**](https://arxiv.org/pdf/2506.09985) | 2025      | [![Star](https://img.shields.io/github/stars/facebookresearch/vjepa2.svg?style=social&label=Star)](https://github.com/facebookresearch/vjepa2) |



### LLM/MLLM

| Title                                                        | Pub.&Date    | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------ | ------------------------------------------------------------ |
| [Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning  **(LLM-DM)**](https://proceedings.neurips.cc/paper_files/paper/2023/file/f9f54762cbb4fe4dbffdd4f792c31221-Paper-Conference.pdf) | NeurIPS 2023 | [![Star](https://img.shields.io/github/stars/GuanSuns/LLMs-World-Models-for-Planning.svg?style=social&label=Star)](https://github.com/GuanSuns/LLMs-World-Models-for-Planning) |
| [WorldGPT: Empowering LLM as Multimodal World Model  **(WorldGPT)**](https://dl.acm.org/doi/pdf/10.1145/3664647.3681488) | ACM MM 2024  | [![Star](https://img.shields.io/github/stars/DCDmllm/WorldGPT.svg?style=social&label=Star)](https://github.com/DCDmllm/WorldGPT) |
| [Text2World: Benchmarking Large Language Models for Symbolic World Model Generation  **(Text2World)**](https://arxiv.org/pdf/2502.13092) | ACL 2025     | [![Star](https://img.shields.io/github/stars/Aaron617/text2world.svg?style=social&label=Star)](https://github.com/Aaron617/text2world) |



### Video generation

| Title                                                        | Pub.&Date    | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------ | ------------------------------------------------------------ |
| [CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers  **(CogVideo)**](https://arxiv.org/pdf/2205.15868) | ICLR 2023    | [![Star](https://img.shields.io/github/stars/zai-org/CogVideo.svg?style=social&label=Star)](https://github.com/zai-org/CogVideo) |
| [Structure and Content-Guided Video Synthesis with Diffusion Models  **(Gen‚Äë1)**](https://arxiv.org/pdf/2508.08086) | ICCV 2023    | [![Website](https://img.shields.io/badge/Website-9cf)](https://runwayml.com/research/gen-1) |
| [UniSim: Learning Interactive Real-World Simulators  **(Unisim)**](https://openreview.net/pdf?id=sFyTZEqmUY) | ICLR 2024    | [![Website](https://img.shields.io/badge/Website-9cf)](https://universal-simulator.github.io/unisim/) |
| [Sora: Creating video from text  **(Sora)**](https://openai.com/sora) | OpenAI 2024  | ‚Äî                                                            |
| [World model on million-length video and language with ring-attention  **(LWM)**](https://arxiv.org/pdf/2402.08268) | ICLR 2025    | [![Star](https://img.shields.io/github/stars/LargeWorldModel/lwm.svg?style=social&label=Star)](https://github.com/LargeWorldModel/lwm) |
| [Genie: Generative Interactive Environmentsn  **(Genie)**](https://openreview.net/pdf?id=bJbSbJskOS) | ICML 2024    | [![Website](https://img.shields.io/badge/Website-9cf)](https://sites.google.com/view/genie-2024/home) |
| [iVideoGPT: Interactive VideoGPTs are Scalable World Models  **(iVideoGPT)**](https://arxiv.org/pdf/2405.15223) | NeurIPS 2024 | [![Star](https://img.shields.io/github/stars/thuml/iVideoGPT.svg?style=social&label=Star)](https://github.com/thuml/iVideoGPT) |
| [CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer  **(CogVideoX)**](https://arxiv.org/pdf/2408.06072) | ICLR 2025    | [![Star](https://img.shields.io/github/stars/zai-org/CogVideo.svg?style=social&label=Star)](https://github.com/zai-org/CogVideo) |
| [Wan: Open and Advanced Large-Scale Video Generative Models  **(Wan)**](https://arxiv.org/pdf/2507.21809) | 2025         | [![Star](https://img.shields.io/github/stars/Wan-Video/Wan2.1.svg?style=social&label=Star)](https://github.com/Wan-Video/Wan2.1) |
| [Cosmos World Foundation Model Platform for Physical AI  **(Cosmos)**](https://arxiv.org/pdf/2501.03575) | 2025         | [![Star](https://img.shields.io/github/stars/nvidia-cosmos/cosmos-predict2.svg?style=social&label=Star)](https://github.com/nvidia-cosmos/cosmos-predict2) |



### Interactive 3D environment

| Title                                                        | Pub.&Date | Code/Project URL                                             |
| ------------------------------------------------------------ | --------- | ------------------------------------------------------------ |
| [Interactive 3D Scene Generation from a Single Image  **(WonderWorld)**](https://arxiv.org/pdf/2308.07540) | CVPR 2025 | [![Star](https://img.shields.io/github/stars/KovenYu/WonderWorld.svg?style=social&label=Star)](https://github.com/KovenYu/WonderWorld) |
| [Matrix-3D: Omnidirectional Explorable 3D World Generation  **(Matrix-3D)**](https://arxiv.org/pdf/2508.08086) | 2025      | [![Star](https://img.shields.io/github/stars/SkyworkAI/Matrix-3D.svg?style=social&label=Star)](https://github.com/SkyworkAI/Matrix-3D) |
| [HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels  **(Hunyuan World)**](https://arxiv.org/pdf/2507.21809) | 2025      | [![Star](https://img.shields.io/github/stars/Tencent-Hunyuan/HunyuanWorld-1.0.svg?style=social&label=Star)](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0) |



### Application

| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |
| [DayDreamer: World Models for Physical Robot Learning  **(DayDreamer)**](https://arxiv.org/pdf/2206.14176) | 2023                | [![Star](https://img.shields.io/github/stars/danijar/daydreamer.svg?style=social&label=Star)](https://github.com/danijar/daydreamer) |
| [Generative Agents: Interactive Simulacra of Human Behavior  **(Generative Agents)**](https://arxiv.org/pdf/2304.03442) | UIST 2023           | [![Star](https://img.shields.io/github/stars/joonspk-research/generative_agents.svg?style=social&label=Star)](https://github.com/joonspk-research/generative_agents) |
| [GAIA-1: A generative world model for autonomous driving  **(GAIA-1)**](https://arxiv.org/pdf/2309.17080) | 2023                | [![Website](https://img.shields.io/badge/Website-9cf)](https://anthonyhu.github.io/gaia1) |
| [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving  **(OccWorld)**](https://arxiv.org/pdf/2311.16038) | ECCV 2024           | [![Star](https://img.shields.io/github/stars/wzzheng/OccWorld.svg?style=social&label=Star)](https://github.com/wzzheng/OccWorld) |
| [Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation  **(GR1)**](https://arxiv.org/pdf/2312.13139) | ICLR 2024           | [![Star](https://img.shields.io/github/stars/bytedance/GR-1.svg?style=social&label=Star)](https://github.com/bytedance/GR-1) |
| [DriveDreamer: Towards real-world-driven world models  **(DriveDreamer)**](https://arxiv.org/pdf/2309.09777) | ECCV 2024           | [![Star](https://img.shields.io/github/stars/JeffWang987/DriveDreamer.svg?style=social&label=Star)](https://github.com/JeffWang987/DriveDreamer) |
| [Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving  **(Drive-WM)**](https://arxiv.org/pdf/2311.17918) | CVPR 2024           | [![Star](https://img.shields.io/github/stars/BraveGroup/Drive-WM.svg?style=social&label=Star)](https://github.com/BraveGroup/Drive-WM) |
| [Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)  **(Think2Drive)**](https://arxiv.org/pdf/2311.17918) | ECCV 2024           | [![Website](https://img.shields.io/badge/Website-9cf)](https://thinklab-sjtu.github.io/CornerCaseRepo/) |
| [Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving  **(Drive-WM)**](https://arxiv.org/pdf/2311.17918) | CVPR 2024           | [![Star](https://img.shields.io/github/stars/BraveGroup/Drive-WM.svg?style=social&label=Star)](https://github.com/BraveGroup/Drive-WM) |
| [GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving  **(GaussianWorld)**](https://arxiv.org/pdf/2412.10373) | CVPR 2025           | [![Star](https://img.shields.io/github/stars/zuosc19/GaussianWorld.svg?style=social&label=Star)](https://github.com/zuosc19/GaussianWorld) |
| [Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving  **(Drive-WM)**](https://arxiv.org/pdf/2311.17918) | 2025                | [![Website](https://img.shields.io/badge/Website-9cf)](https://wayve.ai/thinking/gaia-2/) |
| [World and Human Action Models towards Gameplay Ideation  **(WHAM)**](https://www.nature.com/articles/s41586-025-08600-3.pdf) | Nature 2025         | ‚Äî                                                            |
| [Mineworld: a Real-time and Open-source Interactive World Model on Minecraft  **(MineWorld)**](https://arxiv.org/pdf/2504.08388?) | 2025                | [![Star](https://img.shields.io/github/stars/microsoft/MineWorld.svg?style=social&label=Star)](https://github.com/microsoft/MineWorld) |
| [GameFactory: Creating New Games with Generative Interactive Videos  **(Gamefactory)**](https://arxiv.org/pdf/2501.08325) | ICCV 2025           | [![Star](https://img.shields.io/github/stars/KwaiVGI/GameFactory.svg?style=social&label=Star)](https://github.com/KwaiVGI/GameFactory) |
| [AgentSociety: Large-scale simulation of LLM-driven generative agents  **(AgentSociety)**](https://arxiv.org/pdf/2502.08691) | ACL 2025, COLM 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/AgentSociety.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/AgentSociety) |
| [EnerVerse-AC: Envisioning Embodied Environments with Action Condition  **(EnerVerse)**](https://arxiv.org/pdf/2505.09723) | 2025                | [![Star](https://img.shields.io/github/stars/AgibotTech/EnerVerse-AC.svg?style=social&label=Star)](https://github.com/AgibotTech/EnerVerse-AC) |
| [GR-3 Technical Report  **(GR3)**](https://arxiv.org/pdf/2507.15493) | 2025                | [![Website](https://img.shields.io/badge/Website-9cf)](https://wayve.ai/thinking/gaia-2/) |
| [Aether: Geometric-Aware Unified World Modeling  **(Aether)**](https://arxiv.org/pdf/2503.18945) | 2025                | [![Star](https://img.shields.io/github/stars/InternRobotics/Aether.svg?style=social&label=Star)](https://github.com/InternRobotics/Aether) |
| [GWM: Towards Scalable Gaussian World Models for Robotic Manipulation  **(GWM)**](https://arxiv.org/pdf/2508.17600) | ICCV 2025           | [![Website](https://img.shields.io/badge/Website-9cf)](https://gaussian-world-model.github.io/) |
| [AirScape: An Aerial Generative World Model with Motion Controllability  **(AirScape)**](https://arxiv.org/pdf/2507.08885) | ACM MM 2025         | [![Website](https://img.shields.io/badge/Website-9cf)](https://embodiedcity.github.io/AirScape/) |
| [RoboScape: Physics-informed Embodied World Model  **(RoboScape)**](https://arxiv.org/pdf/2506.23135) | 2025                | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/RoboScape.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/RoboScape) |
| [DreamGen: Unlocking Generalization in Robot Learning through Video World Models  **(DreamGen)**](https://arxiv.org/pdf/2505.12705) | 2025                | [![Star](https://img.shields.io/github/stars/NVIDIA/GR00T-Dreams.svg?style=social&label=Star)](https://github.com/NVIDIA/GR00T-Dreams) |
| [Matrix-Game: Interactive World Foundation Model  **(Matrix-Game)**](https://arxiv.org/pdf/2506.18701) | 2025                | [![Star](https://img.shields.io/github/stars/SkyworkAI/Matrix-Game.svg?style=social&label=Star)](https://github.com/SkyworkAI/Matrix-Game) |
| [Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation  **(Genie Envisioner)**](https://arxiv.org/abs/2508.05635v1) | 2025                | [![Star](https://img.shields.io/github/stars/AgibotTech/Genie-Envisioner.svg?style=social&label=Star)](https://github.com/AgibotTech/Genie-Envisioner) |



## 3 Implicit Representation of the External World

### 3.1 World Model in Decision Making

| Title | Pub. & Date | Code/Project URL |
|---|---|---|
| [Deep reinforcement learning in a handful of trials using probabilistic dynamics models](https://proceedings.neurips.cc/paper_files/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf) | NeurIPS 2018 | [![Star](https://img.shields.io/github/stars/kchua/handful-of-trials.svg?style=social&label=Star)](https://github.com/kchua/handful-of-trials) |
| [PWM: Policy Learning with Multi-Task World Models](https://arxiv.org/pdf/2407.02466) | ICLR 2025 | [![Star](https://img.shields.io/github/stars/imgeorgiev/PWM.svg?style=social&label=Star)](https://github.com/imgeorgiev/PWM) |
| [Recurrent world models facilitate policy evolution](https://proceedings.neurips.cc/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf) | NeurIPS 2018 | [![Website](https://img.shields.io/badge/Website-9cf)](https://worldmodels.github.io/) |
| [Dream to control: Learning behaviors by latent imagination](https://arxiv.org/pdf/1912.01603) | ICLR 2020 | [![Star](https://img.shields.io/github/stars/danijar/dreamer.svg?style=social&label=Star)](https://github.com/danijar/dreamer) |
| [Leveraging pre-trained large language models to construct and utilize world models for model-based task planning](https://arxiv.org/pdf/2305.14909) | NeurIPS 2023 | [![Star](https://img.shields.io/github/stars/GuanSuns/LLMs-World-Models-for-Planning.svg?style=social&label=Star)](https://github.com/GuanSuns/LLMs-World-Models-for-Planning) |
| [Mastering atari with discrete world models](https://arxiv.org/pdf/2010.02193) | ICLR 2021 | [![Star](https://img.shields.io/github/stars/danijar/dreamerv2.svg?style=social&label=Star)](https://github.com/danijar/dreamerv2) |
| [Mastering diverse control tasks through world models](https://arxiv.org/pdf/2301.04104) | Nature 2024 | [![Star](https://img.shields.io/github/stars/danijar/dreamerv3.svg?style=social&label=Star)](https://github.com/danijar/dreamerv3) |
| [TD-MPC2: Scalable, Robust World Models for Continuous Control](https://openreview.net/pdf?id=Oxh5CstDJU) | ICLR 2024 | [![Star](https://img.shields.io/github/stars/nicklashansen/tdmpc2.svg?style=social&label=Star)](https://github.com/nicklashansen/tdmpc2) |
| [When to trust your model: Model-based policy optimization](https://arxiv.org/pdf/1906.08253) | NeurIPS 2019 | [![Star](https://img.shields.io/github/stars/jannerm/mbpo.svg?style=social&label=Star)](https://github.com/jannerm/mbpo) |
| [Offline reinforcement learning as one big sequence modeling problem](https://proceedings.neurips.cc/paper/2021/file/099fe6b0b444c23836c4a5d07346082b-Paper.pdf) | NeurIPS 2021 | [![Star](https://img.shields.io/github/stars/jannerm/trajectory-transformer.svg?style=social&label=Star)](https://github.com/jannerm/trajectory-transformer) |
| [Model predictive control](https://link.springer.com/content/pdf/10.1007/978-3-319-24853-0.pdf) | Springer | ‚Äî |
| [Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees](https://openreview.net/pdf?id=BJe1E2R5KX) | ICLR 2019 | [![Star](https://img.shields.io/github/stars/roosephu/slbo.svg?style=social&label=Star)](https://github.com/roosephu/slbo) |
| [Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning](https://arxiv.org/pdf/1708.02596) | ICRA 2018 | [![Star](https://img.shields.io/github/stars/anagabandi/nn_dynamics.svg?style=social&label=Star)](https://github.com/anagabandi/nn_dynamics) |
| [A game theoretic framework for model based reinforcement learning](https://arxiv.org/pdf/2004.07804) | ICML 2021 | [![Star](https://img.shields.io/github/stars/aravindr93/mjrl.svg?style=social&label=Star)](https://github.com/aravindr93/mjrl/tree/v2) |
| [General agents need world models](https://arxiv.org/pdf/2506.01622) | ICML 2025    | ‚Äî |
| [Mastering memory tasks with world models](https://arxiv.org/abs/2403.04253) | ICLR 2024 | [![Star](https://img.shields.io/github/stars/chandar-lab/Recall2Imagine.svg?style=social&label=Star)](https://github.com/chandar-lab/Recall2Imagine) |
| [A generalist dynamics model for control](https://arxiv.org/abs/2305.10912) | arXiv 2023 | ‚Äî |
| [Exploring model-based planning with policy networks](https://openreview.net/pdf?id=H1exf64KwH) | ICLR 2020 | [![Star](https://img.shields.io/github/stars/WilsonWangTHU/POPLIN.svg?style=social&label=Star)](https://github.com/WilsonWangTHU/POPLIN) |
| [A0c: Alpha zero in continuous action space](https://arxiv.org/abs/1805.09613) | arXiv 2018 | [![Star](https://img.shields.io/github/stars/tmoer/a0c.svg?style=social&label=Star)](https://github.com/tmoer/a0c) |
| [Probabilistic adaptation of text-to-video models](https://arxiv.org/pdf/2306.01872v1) | ICLR 2024 | [![Website](https://img.shields.io/badge/Website-9cf)](https://video-adapter.github.io/video-adapter/) |
| [RoboDreamer: Learning Compositional World Models for Robot Imagination](https://openreview.net/pdf?id=kHjOmAUfVe) | ICML 2024 | [![Star](https://img.shields.io/github/stars/rainbow979/robodreamer.svg?style=social&label=Star)](https://github.com/rainbow979/robodreamer) |
| [Discuss before moving: Visual language navigation via multi-expert discussions](https://arxiv.org/pdf/2309.11382) | ICRA 2024 | [![Star](https://img.shields.io/github/stars/LYX0501/DiscussNav.svg?style=social&label=Star)](https://github.com/LYX0501/DiscussNav) |
| [OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and Structured Representation](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_OVER-NAV_Elevating_Iterative_Vision-and-Language_Navigation_with_Open-Vocabulary_Detection_and_StructurEd_CVPR_2024_paper.pdf) | CVPR 2024 | [![Star](https://img.shields.io/github/stars/uitrbn/OVER-NAV-IVLN-CE.svg?style=social&label=Star)](https://github.com/uitrbn/OVER-NAV-IVLN-CE) |
| [RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation](https://openaccess.thecvf.com/content/CVPR2024//papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf) | CVPR 2024 | [![Website](https://img.shields.io/badge/Website-9cf)](https://rila-savn.github.io/RILA/) |
| [Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models](https://arxiv.org/pdf/2501.09686) | arXiv 2025 | ‚Äî |
| [Position: LLMs can't plan, but can help planning in LLM-modulo frameworks](https://arxiv.org/pdf/2402.01817) | ICML 2024 | ‚Äî |
| [Language models meet world models: Embodied experiences enhance language models](https://proceedings.neurips.cc/paper_files/paper/2023/file/ee6630dcbcff857026e474fc857aa9f0-Paper-Conference.pdf#:~:text=In%20this%20paper%2C%20we%20propose%20a%20new%20paradigm,embodied%20knowledge%20while%20retaining%20their%20general%20language%20capabilities.) | NeurIPS 2023 | [![Star](https://img.shields.io/github/stars/szxiangjn/world-model-for-language-model.svg?style=social&label=Star)](https://github.com/szxiangjn/world-model-for-language-model) |
| [Virtualhome: Simulating household activities via programs](https://openaccess.thecvf.com/content_cvpr_2018/papers/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.pdf) | CVPR 2018 | [![Star](https://img.shields.io/github/stars/xavierpuigf/virtualhome.svg?style=social&label=Star)](https://github.com/xavierpuigf/virtualhome) |
| [Learning to Model the World with Language](https://arxiv.org/pdf/2308.01399) | ICML 2024 | [![Star](https://img.shields.io/github/stars/jlin816/dynalang.svg?style=social&label=Star)](https://github.com/jlin816/dynalang) |
| [Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency](https://arxiv.org/abs/2309.17382) | ICML 2024 | [![Star](https://img.shields.io/github/stars/agentification/RAFA_code.svg?style=social&label=Star)](https://github.com/agentification/RAFA_code) |
| [Alfworld: Aligning text and embodied environments for interactive learning](https://arxiv.org/pdf/2010.03768) | ICLR 2021 | [![Star](https://img.shields.io/github/stars/alfworld/alfworld.svg?style=social&label=Star)](https://github.com/alfworld/alfworld) |
| [Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents](https://arxiv.org/abs/2411.06559) | EMNLP 2024 | [![Star](https://img.shields.io/github/stars/OSU-NLP-Group/WebDreamer.svg?style=social&label=Star)](https://github.com/OSU-NLP-Group/WebDreamer) |
| [Agent Planning with World Knowledge Model](https://arxiv.org/pdf/2405.14205) | NeurIPS 2024 | [![Star](https://img.shields.io/github/stars/zjunlp/WKM.svg?style=social&label=Star)](https://github.com/zjunlp/WKM) |
| [WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment](https://arxiv.org/pdf/2402.12275) | NeurIPS 2024 | [![Star](https://img.shields.io/github/stars/haotang1995/WorldCoder.svg?style=social&label=Star)](https://github.com/haotang1995/WorldCoder) |
| [Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation](https://arxiv.org/pdf/2410.13232) | ICLR 2025 | [![Star](https://img.shields.io/github/stars/kyle8581/WMA-Agents.svg?style=social&label=Star)](https://github.com/kyle8581/WMA-Agents) |



### 3.2 World Knowledge Learned by Models

| Title | Pub. & Date | Code / Project URL |
|---|---|---|
| [Does the chimpanzee have a theory of mind?](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/1E96B02CD9850016B7C93BC6D2FEF1D0/S0140525X00076512a.pdf/does-the-chimpanzee-have-a-theory-of-mind.pdf) | Behav. & Brain Sci. 1978 | ‚Äî |
| [GPT4GEO: How a Language Model Sees the World‚Äôs Geography](https://arxiv.org/pdf/2306.00020) | NeurIPS  2023 | [![Star](https://img.shields.io/github/stars/jonathan-roberts1/GPT4GEO.svg?style=social&label=Star)](https://github.com/jonathan-roberts1/GPT4GEO) |
| [LLMs achieve adult human performance on higher-order theory of mind tasks](https://arxiv.org/pdf/2405.18870) | arXiv 2024 | ‚Äî |
| [COKE: A cognitive knowledge graph for machine theory of mind](https://aclanthology.org/2024.acl-long.848.pdf) | ACL 2024 | [![Star](https://img.shields.io/github/stars/jincenziwu/COKE.svg?style=social&label=Star)](https://github.com/jincenziwu/COKE) |
| [Think Twice: Perspective-Taking Improves LLM Theory-of-Mind](https://arxiv.org/pdf/2311.10227) | ACL 2024 | [![Star](https://img.shields.io/github/stars/shawnsihyunlee/simulatedtom.svg?style=social&label=Star)](https://github.com/shawnsihyunlee/simulatedtom) |
| [Language Models Represent Space and Time](https://arxiv.org/pdf/2310.02207) | ICLR 2024 | [![Star](https://img.shields.io/github/stars/wesg52/world-models.svg?style=social&label=Star)](https://github.com/wesg52/world-models) |
| [GeoLLM: Extracting Geospatial Knowledge from Large Language Models](https://arxiv.org/pdf/2310.06213) | ICLR 2024 | [![Star](https://img.shields.io/github/stars/rohinmanvi/GeoLLM.svg?style=social&label=Star)](https://github.com/rohinmanvi/GeoLLM) |
| [Large language models are geographically biased](https://arxiv.org/pdf/2402.02680) | ICML 2024 | [![Star](https://img.shields.io/github/stars/rohinmanvi/GeoLLM.svg?style=social&label=Star)](https://github.com/rohinmanvi/GeoLLM) |
| [Emergent Representations of Program Semantics in Language Models Trained on Programs](https://openreview.net/pdf?id=zRrzSLwNHQ) | ICML 2024 | [![Star](https://img.shields.io/github/stars/charlesjin/emergent-semantics.svg?style=social&label=Star)](https://github.com/charlesjin/emergent-semantics) |
| [BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages](https://arxiv.org/pdf/2406.09948) | NeurIPS 2024 | [![Star](https://img.shields.io/github/stars/nlee0212/BLEnD.svg?style=social&label=Star)](https://github.com/nlee0212/BLEnD) |
| [SafeWorld: Geo-Diverse Safety Alignment](https://arxiv.org/pdf/2412.06483) | NeurIPS 2024 | [![Star](https://img.shields.io/github/stars/PlusLabNLP/SafeWorld.svg?style=social&label=Star)](https://github.com/PlusLabNLP/SafeWorld) |
| [EAI: Emotional Decision-Making of LLMs in Strategic Games and Ethical Dilemmas](https://openreview.net/forum?id=8aAaYEwNR4) | NeurIPS 2024 | [![Star](https://img.shields.io/github/stars/AIRI-Institute/EAI-Framework.svg?style=social&label=Star)](https://github.com/AIRI-Institute/EAI-Framework) |
| [Testing theory of mind in large language models and humans](https://www.nature.com/articles/s41562-024-01882-z) | Nature Human Behaviour 2024 | [Website](https://osf.io/j3vhq) |
| [Automated construction of cognitive maps with visual predictive coding](https://www.nature.com/articles/s42256-024-00863-1) | Nature Machine Intelligence 2024 | [![Star](https://img.shields.io/github/stars/jgornet/predictive-coding-recovers-maps.svg?style=social&label=Star)](https://github.com/jgornet/predictive-coding-recovers-maps) |
| [Evaluating Large Language Models in Theory of Mind Tasks](https://www.pnas.org/doi/10.1073/pnas.2405460121) | PNAS 2024 | [Website](https://osf.io/csdhb/) |
| [Elements of World Knowledge (EWOK)](https://arxiv.org/pdf/2405.09605) | Transactions of the ACL 2025 | [Website](https://ewok-core.github.io/) |
| [The Geometry of Concepts: Sparse Autoencoder Feature Structure](https://arxiv.org/abs/2410.19750) | Entropy 2025 | [![Star](https://img.shields.io/github/stars/ejmichaud/feature-geometry.svg?style=social&label=Star)](https://github.com/ejmichaud/feature-geometry) |
| [AgentMove: A large language model based agentic framework for zero-shot next location prediction](https://arxiv.org/abs/2408.13986) | NAACL 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/AgentMove.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/AgentMove) |
| [CityGPT: Empowering Urban Spatial Cognition of Large Language Models](https://arxiv.org/abs/2406.13948) | KDD 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/CityGPT.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/CityGPT) |
| [CityBench: Evaluating the Capabilities of Large Language Model as World Model](https://arxiv.org/abs/2406.13945) | KDD 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/CityBench.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/CityBench) |
| [LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services](https://www.arxiv.org/abs/2506.02720) | KDD 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/LocalEval.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/LocalEval) |
| [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence](https://arxiv.org/abs/2506.23219) | ICCV 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/UrbanLLaVA.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/UrbanLLaVA) |
| [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/abs/2506.02713) | ACL 2025 Findings | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/PIGEON.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/PIGEON) |
| [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876) | ACL 2025 Findings | [Website](https://wm-abench.maitrix.org/) |
| [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586) | EMNLP 2025 Findings |[![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/GeospatialHallucination.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/GeospatialHallucination) |
| [GPS as a Control Signal for Image Generation](https://arxiv.org/abs/2501.12390) | CVPR 2025 | [![Star](https://img.shields.io/github/stars/cfeng16/GPS2Pix.svg?style=social&label=Star)](https://github.com/cfeng16/GPS2Pix) |
| [All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages](https://arxiv.org/abs/2411.16508) | CVPR 2025 | [Website](https://mbzuai-oryx.github.io/ALM-Bench/) |
| [Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models](https://arxiv.org/abs/2502.08636) | CVPR 2025 | [![Star](https://img.shields.io/github/stars/XingruiWang/Spatial457.svg?style=social&label=Star)](https://github.com/XingruiWang/Spatial457) |
| [Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces](https://arxiv.org/abs/2412.14171) | CVPR 2025 | [Website](https://vision-x-nyu.github.io/thinking-in-space.github.io/) |
| [A Survey of Large Language Model-Powered Spatial Intelligence Across Scales](https://arxiv.org/abs/2504.09848) | arXiv 2025 | ‚Äî |
| [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/pdf/2506.16898) | arXiv 2025 | ‚Äî |
| [Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models](https://arxiv.org/pdf/2506.14674) | arXiv 2025 | ‚Äî |

## 4 Future Prediction of the Physical World

### 4.1 World Model as Video Generation

| Title                                                        | Pub. & Date      | Code / Project URL                                           |
| ------------------------------------------------------------ | ---------------- | ------------------------------------------------------------ |
| [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators) | OpenAI Blog 2024 | ‚Äî                                                            |
| [Sora: Creating video from text](https://openai.com/sora)    | OpenAI 2024      | ‚Äî                                                            |
| [Is Sora a world simulator? A comprehensive survey on general world models and beyond](https://arxiv.org/pdf/2405.03520) | arXiv 2024       | [![Star](https://img.shields.io/github/stars/GigaAI-research/General-World-Models-Survey.svg?style=social&label=Star)](https://github.com/GigaAI-research/General-World-Models-Survey) |
| [Sora as an AGI world model? A complete survey on text-to-video generation](https://arxiv.org/pdf/2403.05131) | arXiv 2024       | ‚Äî                                                            |
| [How Far is Video Generation from World Model: A Physical Law Perspective](https://arxiv.org/pdf/2411.02385) | ICML 2025        | [![Star](https://img.shields.io/github/stars/phyworld/phyworld.svg?style=social&label=Star)](https://github.com/phyworld/phyworld) |
| [Do generative video models learn physical principles from watching videos?](https://arxiv.org/pdf/2501.09038) | arXiv 2025       | [![Star](https://img.shields.io/github/stars/google-deepmind/physics-IQ-benchmark.svg?style=social&label=Star)](https://github.com/google-deepmind/physics-IQ-benchmark) |
| [Genesis: A Generative and Universal Physics Engine for Robotics and Beyond](https://arxiv.org/pdf/2405.18556) | ICML 2024        | [![Star](https://img.shields.io/github/stars/Genesis-Embodied-AI/Genesis.svg?style=social&label=Star)](https://github.com/Genesis-Embodied-AI/Genesis) |
| [PhysGen: Rigid-body physics-grounded image-to-video generation](https://arxiv.org/pdf/2409.18964) | ECCV 2024        | [![Star](https://img.shields.io/github/stars/stevenlsw/physgen.svg?style=social&label=Star)](https://github.com/stevenlsw/physgen) |
| [NUWA-XL: Diffusion over Diffusion for Extremely Long Video Generation](https://arxiv.org/pdf/2303.12346) | ACL 2023         | [Website](https://msra-nuwa.azurewebsites.net/#/)            |
| [OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/pdf/2311.16038) | ECCV 2024        | [![Star](https://img.shields.io/github/stars/wzzheng/OccWorld.svg?style=social&label=Star)](https://github.com/wzzheng/OccWorld) |
| [OccSora: 4D Occupancy Generation Models as World Simulators](https://openreview.net/pdf?id=XIFnghzusY) | ICLR 2025        | [![Star](https://img.shields.io/github/stars/wzzheng/OccSora.svg?style=social&label=Star)](https://github.com/wzzheng/OccSora) |
| [World model on million-length video and language with ring-attention](https://arxiv.org/pdf/2402.08268) | ICLR 2025        | [![Star](https://img.shields.io/github/stars/LargeWorldModel/lwm.svg?style=social&label=Star)](https://github.com/LargeWorldModel/lwm) |
| [GAIA-1: A generative world model for autonomous driving](https://arxiv.org/pdf/2309.17080) | arXiv 2023       | [Website](https://anthonyhu.github.io/gaia1)                 |
| [DriveDreamer: Towards real-world-driven world models](https://arxiv.org/pdf/2309.09777) | ECCV 2024        | [![Star](https://img.shields.io/github/stars/JeffWang987/DriveDreamer.svg?style=social&label=Star)](https://github.com/JeffWang987/DriveDreamer) |
| [DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation](https://arxiv.org/pad/2403.06845) | AAAI 2025        | [![Star](https://img.shields.io/github/stars/f1yfisher/DriveDreamer2.svg?style=social&label=Star)](https://github.com/f1yfisher/DriveDreamer2) |
| [Driving into the Future: Multiview Visual Forecasting and Planning with World Model](https://arxiv.org/pdf/2311.17918) | CVPR 2024        | [![Star](https://img.shields.io/github/stars/BraveGroup/Drive-WM.svg?style=social&label=Star)](https://github.com/BraveGroup/Drive-WM) |
| [Vista: A Generalizable Driving World Model with High Fidelity](https://arxiv.org/pdf/2405.17398) | NeurIPS 2024     | [![Star](https://img.shields.io/github/stars/OpenDriveLab/Vista.svg?style=social&label=Star)](https://github.com/OpenDriveLab/Vista) |
| [WorldDreamer: Towards general world models for video generation](https://arxiv.org/abs/2401.09985) | arXiv 2024       | [![Star](https://img.shields.io/github/stars/JeffWang987/WorldDreamer.svg?style=social&label=Star)](https://github.com/JeffWang987/WorldDreamer) |
| [WorldGPT: a Sora-inspired video AI agent](https://arxiv.org/abs/2403.07944) | arXiv 2024       | ‚Äî                                                            |


### 4.2 World Model as Embodied Environment

| Title | Pub. & Date | Code / Project URL |
|---|---|---|
| [Holodeck: Language guided generation of 3d embodied ai environments](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Holodeck_Language_Guided_Generation_of_3D_Embodied_AI_Environments_CVPR_2024_paper.pdf) | CVPR 2024 | [![Star](https://img.shields.io/github/stars/allenai/Holodeck.svg?style=social&label=Star)](https://github.com/allenai/Holodeck) |
| [GRUtopia: Dream General Robots in a City at Scale](https://arxiv.org/abs/2407.10943) | arXiv 2024 | [![Star](https://img.shields.io/github/stars/InternRobotics/InternUtopia.svg?style=social&label=Star)](https://github.com/InternRobotics/InternUtopia) |
| [Anyhome: Open-vocabulary generation of structured and textured 3d homes](https://arxiv.org/abs/2312.06644) | ECCV 2024 | [![Star](https://img.shields.io/github/stars/FreddieRao/anyhome_github.svg?style=social&label=Star)](https://github.com/FreddieRao/anyhome_github) |
| [LEGENT: Open Platform for Embodied Agents](https://arxiv.org/abs/2404.18243) | arXiv 2024 | [![Star](https://img.shields.io/github/stars/thunlp/LEGENT.svg?style=social&label=Star)](https://github.com/thunlp/LEGENT) |
| [UrbanWorld: An Urban World Model for 3D City Generation](https://arxiv.org/abs/2407.11965) | arXiv 2024 | [![Star](https://img.shields.io/github/stars/Urban-World/UrbanWorld.svg?style=social&label=Star)](https://github.com/Urban-World/UrbanWorld) |
| [MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility](https://arxiv.org/pdf/2407.08725) | ICLR 2025 | [![Star](https://img.shields.io/github/stars/metadriverse/metaurban.svg?style=social&label=Star)](https://github.com/metadriverse/metaurban) |
| [Minedojo: Building open-ended embodied agents with internet-scale knowledge](https://arxiv.org/abs/2206.08853) | NeurIPS 2022 | [![Star](https://img.shields.io/github/stars/MineDojo/MineDojo.svg?style=social&label=Star)](https://github.com/MineDojo/MineDojo) |
| [UniSim: Learning Interactive Real-World Simulators](https://openreview.net/pdf?id=sFyTZEqmUY) | ICLR 2024 | [![Website](https://img.shields.io/badge/Website-9cf)](https://universal-simulator.github.io/unisim/) |
| [EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment](https://arxiv.org/abs/2410.09604) | arXiv 2024 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/EmbodiedCity.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/EmbodiedCity) |
| [Empowering World Models with Reflection for Embodied Video Prediction](https://arxiv.org/abs/2410.15461) | ICML 2025 | ‚Äî |
| [Streetscapes: Large-scale consistent street view generation using autoregressive video diffusion](https://arxiv.org/abs/2406.03578) | SIGGRAPH 2024 | ‚Äî |
| [AVID: Adapting Video Diffusion Models to World Models](https://arxiv.org/abs/2410.12822) | arXiv 2024 | [![Star](https://img.shields.io/github/stars/microsoft/causica.svg?style=social&label=Star)](https://github.com/microsoft/causica/tree/main/research_experiments/avid) |
| [Pandora: Towards General World Model with Natural Language Actions and Video States](https://arxiv.org/abs/2406.09455) | arXiv 2024 | [![Star](https://img.shields.io/github/stars/maitrix-org/Pandora.svg?style=social&label=Star)](https://github.com/maitrix-org/Pandora) |
| [RoboScape: Physics-informed Embodied World Model](https://arxiv.org/abs/2506.23135) | arXiv 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/RoboScape.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/RoboScape) |
| [TesserAct: Learning 4D Embodied World Models](https://arxiv.org/abs/2504.20995) | arXiv 2025 | [![Star](https://img.shields.io/github/stars/UMass-Embodied-AGI/TesserAct.svg?style=social&label=Star)](https://github.com/UMass-Embodied-AGI/TesserAct) |

## 5 Applications of World Models

### 5.1 Game Intelligence

| Title                                                        | Pub. & Date | Code / Project URL                                           |
| ------------------------------------------------------------ | ----------- | ------------------------------------------------------------ |
| [World and Human Action Models towards Gameplay Ideation](https://www.nature.com/articles/s41586-025-08600-3.pdf) | Nature 2025 | ‚Äî                                                            |
| [GameFactory: Creating New Games with Generative Interactive Videos](https://arxiv.org/pdf/2501.08325) | ICCV 2025   | [![Star](https://img.shields.io/github/stars/KwaiVGI/GameFactory.svg?style=social&label=Star)](https://github.com/KwaiVGI/GameFactory) |
| [Unbounded: A Generative Infinite Game of Character Life Simulation](https://arxiv.org/pdf/2410.18975) | CVPR 2025   | [Website](https://generative-infinite-game.github.io/)       |
| [GameGen-ùïè: Interactive Open-world Game Video Generation](https://arxiv.org/pdf/2411.00769) | ICLR 2025   | [![Star](https://img.shields.io/github/stars/GameGen-X/GameGen-X.svg?style=social&label=Star)](https://github.com/GameGen-X/GameGen-X) |
| [Diffusion Models Are Real-Time Game Engines](https://arxiv.org/pdf/2408.14837?) | ICLR 2025   | [Website](https://gamengen.github.io/)                       |
| [Exploration-Driven Generative Interactive Environments](https://arxiv.org/pdf/2506.09985) | ICLR 2025   | [![Star](https://img.shields.io/github/stars/facebookresearch/vjepa2.svg?style=social&label=Star)](https://github.com/facebookresearch/vjepa2) |
| [Matrix-Game: Interactive World Foundation Model](https://arxiv.org/pdf/2506.18701) | arXiv 2025  | [![Star](https://img.shields.io/github/stars/SkyworkAI/Matrix-Game.svg?style=social&label=Star)](https://github.com/SkyworkAI/Matrix-Game) |
| [Mineworld: a Real-time and Open-source Interactive World Model on Minecraft](https://arxiv.org/pdf/2504.08388?) | arXiv 2025  | [![Star](https://img.shields.io/github/stars/microsoft/MineWorld.svg?style=social&label=Star)](https://github.com/microsoft/MineWorld) |
| [Model as a Game: On Numerical and Spatial Consistency for Generative Games](https://arxiv.org/pdf/2503.21172?) | arXiv 2025  | ‚Äî                                                            |



### 5.2 Embodied Intelligence

| Title                                                        | Pub. & Date  | Code / Project URL                                           |
| ------------------------------------------------------------ | ------------ | ------------------------------------------------------------ |
| [OpenEQA: Embodied Question Answering in the Era of Foundation Models](https://open-eqa.github.io/assets/pdfs/paper.pdf) | CVPR 2024    | [![Star](https://img.shields.io/github/stars/facebookresearch/open-eqa.svg?style=social&label=Star)](https://github.com/facebookresearch/open-eqa) |
| [iVideoGPT: Interactive VideoGPTs are Scalable World Models](https://arxiv.org/abs/2405.15223) | NeurIPS 2024 | [![Star](https://img.shields.io/github/stars/thuml/iVideoGPT.svg?style=social&label=Star)](https://github.com/thuml/iVideoGPT) |
| [IRASim: A Fine-Grained World Model for Robot Manipulation](https://arxiv.org/abs/2406.14540) | ICCV 2025    | [![Star](https://img.shields.io/github/stars/bytedance/IRASim.svg?style=social&label=Star)](https://github.com/bytedance/IRASim) |
| [RoboScape: Physics-informed Embodied World Model](https://arxiv.org/abs/2506.23135) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/RoboScape.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/RoboScape) |
| [TesserAct: Learning 4D Embodied World Models](https://arxiv.org/abs/2504.20995) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/UMass-Embodied-AGI/TesserAct.svg?style=social&label=Star)](https://github.com/UMass-Embodied-AGI/TesserAct) |
| [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/facebookresearch/vjepa2.svg?style=social&label=Star)](https://github.com/facebookresearch/vjepa2) |
| [Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations](https://arxiv.org/abs/2412.14803) | ICML 2025    | [![Star](https://img.shields.io/github/stars/roboterax/video-prediction-policy.svg?style=social&label=Star)](https://github.com/roboterax/video-prediction-policy) |
| [DreamGen: Unlocking Generalization in Robot Learning through Video World Models](https://arxiv.org/abs/2505.12705) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/NVIDIA/GR00T-Dreams.svg?style=social&label=Star)](https://github.com/NVIDIA/GR00T-Dreams) |
| [EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation](https://arxiv.org/abs/2501.01895) | arXiv 2025   | [Website](https://sites.google.com/view/enerverse)           |
| [EnerVerse-AC: Envisioning Embodied Environments with Action Condition](https://arxiv.org/abs/2505.09723) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/AgibotTech/EnerVerse-AC.svg?style=social&label=Star)](https://github.com/AgibotTech/EnerVerse-AC) |
| [Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation](https://arxiv.org/abs/2508.05635v1) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/AgibotTech/Genie-Envisioner.svg?style=social&label=Star)](https://github.com/AgibotTech/Genie-Envisioner) |
| [Vidar: Embodied Video Diffusion Model for Generalist Bimanual Manipulation](https://arxiv.org/abs/2507.12898) | arXiv 2025   | ‚Äî                                                            |
| [WorldVLA: Towards Autoregressive Action World Model](https://arxiv.org/abs/2506.21539) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/alibaba-damo-academy/WorldVLA.svg?style=social&label=Star)](https://github.com/alibaba-damo-academy/WorldVLA) |
| [ManiGaussian++: General Robotic Bimanual Manipulation with Hierarchical Gaussian World Model](https://arxiv.org/pdf/2506.19842) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/April-Yz/ManiGaussian_Bimanual.svg?style=social&label=Star)](https://github.com/April-Yz/ManiGaussian_Bimanual) |
| [ORV: 4D Occupancy-centric Robot Video Generation](https://arxiv.org/abs/2506.03079) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/OrangeSodahub/ORV.svg?style=social&label=Star)](https://github.com/OrangeSodahub/ORV) |
| [GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](https://arxiv.org/pdf/2508.17600) | ICCV 2025    | [Website](https://gaussian-world-model.github.io/)           |
| [WorldEval: World Model as Real-World Robot Policies Evaluator](https://arxiv.org/abs/2505.19017) | arXiv 2025   | [![Star](https://img.shields.io/github/stars/liyaxuanliyaxuan/Worldeval.svg?style=social&label=Star)](https://github.com/liyaxuanliyaxuan/Worldeval) |



### 5.3 Urban Intelligence

#### Autonomous Driving

| Title | Pub. & Date | Code / Project URL |
|---|---|---|
| [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators) | OpenAI Research (2024) | [Website](https://openai.com/research/video-generation-models-as-world-simulators) |
| [GPT-4 technical report](https://arxiv.org/abs/2303.08774) | arXiv 2023 | ‚Äî |
| [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) | NeurIPS 2023 | [![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)](https://github.com/haotian-liu/LLaVA) |
| [World models for autonomous driving: An initial survey](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10522953) | IEEE T-IV 2024 | ‚Äî |
| [Waymax: An accelerated, data-driven simulator for large-scale autonomous driving research](https://arxiv.org/pdf/2310.17918) | arXiv 2023 | [![Star](https://img.shields.io/github/stars/waymo-research/waymax.svg?style=social&label=Star)](https://github.com/waymo-research/waymax) |
| [Planning-oriented autonomous driving](https://arxiv.org/abs/2212.10156) | CVPR 2023 | [![Star](https://img.shields.io/github/stars/OpenDriveLab/UniAD.svg?style=social&label=Star)](https://github.com/OpenDriveLab/UniAD) |
| [A survey on trajectory-prediction methods for autonomous driving](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9779556) | IEEE T-IV 2022 | ‚Äî |
| [BEVFormer: Learning bird‚Äôs-eye-view representation from multi-camera images via spatiotemporal transformers](https://arxiv.org/abs/2203.17270) | ECCV 2022 | [![Star](https://img.shields.io/github/stars/fundamentalvision/BEVFormer.svg?style=social&label=Star)](https://github.com/fundamentalvision/BEVFormer) |
| [Transfusion: Robust lidar-camera fusion for 3D object detection with transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.pdf) | CVPR 2022 | [![Star](https://img.shields.io/github/stars/XuyangBai/TransFusion.svg?style=social&label=Star)](https://github.com/XuyangBai/TransFusion) |
| [YOLOP: You only look once for panoptic driving perception](https://link.springer.com/article/10.1007/s11633-022-1339-y) | MIR 2022 | [![Star](https://img.shields.io/github/stars/hustvl/YOLOP.svg?style=social&label=Star)](https://github.com/hustvl/YOLOP) |
| [Wayformer: Motion forecasting via simple & efficient attention networks](https://arxiv.org/abs/2207.05844) | ICRA 2023 | ‚Äî |
| [Motion Transformer with Global Intention Localization and Local Movement Refinement](https://papers.nips.cc/paper_files/paper/2022/file/2ab47c960bfee4f86dfc362f26ad066a-Paper-Conference.pdf) | NeurIPS 2022 | [![Star](https://img.shields.io/github/stars/sshaoshuai/MTR.svg?style=social&label=Star)](https://github.com/sshaoshuai/MTR) |
| [Query-Centric Trajectory Prediction](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Query-Centric_Trajectory_Prediction_CVPR_2023_paper.pdf) | CVPR 2023 | [![Star](https://img.shields.io/github/stars/ZikangZhou/QCNet.svg?style=social&label=Star)](https://github.com/ZikangZhou/QCNet) |
| [HPTR: Real-time motion prediction via heterogeneous polyline transformer with relative pose encoding](https://arxiv.org/abs/2310.12970) | NeurIPS 2023 | [![Star](https://img.shields.io/github/stars/zhejz/HPTR.svg?style=social&label=Star)](https://github.com/zhejz/HPTR) |
| [MotionDiffuser: Controllable multi-agent motion prediction using diffusion](https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_MotionDiffuser_Controllable_Multi-Agent_Motion_Prediction_Using_Diffusion_CVPR_2023_paper.pdf) | CVPR 2023 | ‚Äî |
| [Tokenize the world into object-level knowledge to address long-tail events in autonomous driving](https://arxiv.org/abs/2407.00959) | arXiv 2024 | ‚Äî |
| [GAIA-1: A generative world model for autonomous driving](https://arxiv.org/pdf/2309.17080) | arXiv 2023             | [Website](https://anthonyhu.github.io/gaia1)                 |
| [DriveDreamer: Towards real-world-driven world models for autonomous driving](https://arxiv.org/abs/2309.09777) | ECCV 2024 | [![Star](https://img.shields.io/github/stars/JeffWang987/DriveDreamer.svg?style=social&label=Star)](https://github.com/JeffWang987/DriveDreamer) |
| [Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving](https://arxiv.org/abs/2311.17918) | CVPR 2024 | [![Star](https://img.shields.io/github/stars/BraveGroup/Drive-WM.svg?style=social&label=Star)](https://github.com/BraveGroup/Drive-WM) |
| [OccWorld: Learning a 3D occupancy world model for autonomous driving](https://arxiv.org/abs/2311.16038) | ECCV 2024 | [![Star](https://img.shields.io/github/stars/wzzheng/OccWorld.svg?style=social&label=Star)](https://github.com/wzzheng/OccWorld) |
| [OccSora: 4D occupancy generation models as world simulators for autonomous driving](https://arxiv.org/abs/2405.20337) | arXiv 2024 | [![Star](https://img.shields.io/github/stars/wzzheng/OccSora.svg?style=social&label=Star)](https://github.com/wzzheng/OccSora) |
| [Vista: A generalizable driving world model with high fidelity and versatile controllability](https://arxiv.org/abs/2405.17398) | NeurIPS 2024 | [![Star](https://img.shields.io/github/stars/OpenDriveLab/Vista.svg?style=social&label=Star)](https://github.com/OpenDriveLab/Vista) |
| [Copilot4D: Learning unsupervised world models for autonomous driving via discrete diffusion](https://arxiv.org/abs/2311.01017) | ICLR 2024 | ‚Äî |
| [MUVO: A multimodal generative world model for autonomous driving with geometric representations](https://arxiv.org/abs/2311.11762) | IEEE T-IV 2025 | [![Star](https://img.shields.io/github/stars/fzi-forschungszentrum-informatik/muvo.svg?style=social&label=Star)](https://github.com/fzi-forschungszentrum-informatik/muvo) |
| [UniWorld: Autonomous driving pre-training via world models](https://arxiv.org/abs/2308.07234) | arXiv 2023 | [![Star](https://img.shields.io/github/stars/whuhxb/UniWorld.svg?style=social&label=Star)](https://github.com/whuhxb/UniWorld) |
| [MetaUrban: A simulation platform for embodied AI in urban spaces](https://arxiv.org/abs/2407.08725) | ICLR 2025 | [![Star](https://img.shields.io/github/stars/metadriverse/MetaUrban.svg?style=social&label=Star)](https://github.com/metadriverse/MetaUrban) |
| [UrbanWorld: An urban world model for 3D city generation](https://arxiv.org/abs/2407.11965) | arXiv 2024 | [![Star](https://img.shields.io/github/stars/Urban-World/UrbanWorld.svg?style=social&label=Star)](https://github.com/Urban-World/UrbanWorld) |
| [Streetscapes: Large-scale consistent street view generation using autoregressive video diffusion](https://arxiv.org/pdf/2407.13759) | SIGGRAPH 2024 | [Website](https://arxiv.org/pdf/2407.13759) |

#### Autonomous Logistics & Urban Analytics

| Title | Pub. & Date | Code / Project URL |
|---|---|---|
| [Navigation World Models](https://arxiv.org/abs/2412.03572) | CVPR 2025 | [![Star](https://img.shields.io/github/stars/facebookresearch/nwm.svg?style=social&label=Star)](https://github.com/facebookresearch/nwm) |
| [Towards Autonomous Micromobility through Scalable Urban Simulation](https://arxiv.org/abs/2505.00690) | CVPR 2025 | [![Star](https://img.shields.io/github/stars/metadriverse/urban-sim.svg?style=social&label=Star)](https://github.com/metadriverse/urban-sim) |
| [Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation](https://arxiv.org/abs/2501.06693) | CVPR 2025 | [![Star](https://img.shields.io/github/stars/Vid2Sim/Vid2Sim.svg?style=social&label=Star)](https://github.com/Vid2Sim/Vid2Sim) |
| [CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos](https://arxiv.org/abs/2411.17820) | CVPR 2025 | [![Star](https://img.shields.io/github/stars/ai4ce/CityWalker.svg?style=social&label=Star)](https://github.com/ai4ce/CityWalker) |
| [AirScape: An Aerial Generative World Model with Motion Controllability  **(AirScape)**](https://arxiv.org/pdf/2507.08885) | ACM MM 2025 | [![Website](https://img.shields.io/badge/Website-9cf)](https://embodiedcity.github.io/AirScape/) |
| [CityNavAgent: Aerial Vision-and-Language Navigation with Hierarchical Semantic Planning and Global Memory](https://arxiv.org/abs/2505.05622) | ACL 2025 | [![Star](https://img.shields.io/github/stars/EmbodiedCity/CityNavAgent.code.svg?style=social&label=Star)](https://github.com/EmbodiedCity/CityNavAgent.code) |
| [CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space](https://arxiv.org/abs/2502.12532) | EMNLP 2025 | [![Star](https://img.shields.io/github/stars/BiluYong/CityEQA.svg?style=social&label=Star)](https://github.com/BiluYong/CityEQA) |
| [UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces](https://arxiv.org/abs/2503.06157) | ACL 2025 | [![Star](https://img.shields.io/github/stars/EmbodiedCity/UrbanVideo-Bench.code.svg?style=social&label=Star)](https://github.com/EmbodiedCity/UrbanVideo-Bench.code) |
| [GeoLLM: Extracting Geospatial Knowledge from Large Language Models](https://arxiv.org/abs/2310.06213) | ICLR 2024 | [![Star](https://img.shields.io/github/stars/rohinmanvi/GeoLLM.svg?style=social&label=Star)](https://github.com/rohinmanvi/GeoLLM) |
| [CityGPT: Empowering Urban Spatial Cognition of Large Language Models](https://arxiv.org/abs/2406.13948) | KDD 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/CityGPT.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/CityGPT) |
| [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence](https://arxiv.org/abs/2506.23219) | ICCV 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/UrbanLLaVA.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/UrbanLLaVA) |
| [GPS as a Control Signal for Image Generation](https://arxiv.org/abs/2501.12390) | CVPR 2025 | [![Star](https://img.shields.io/github/stars/cfeng16/GPS2Pix.svg?style=social&label=Star)](https://github.com/cfeng16/GPS2Pix) |
| [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898) | arXiv 2025 | ‚Äî |
| [AgentMove: A Large Language Model based Agentic Framework for Zero-shot Next Location Prediction](https://aclanthology.org/2025.naacl-long.61/) | NAACL 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/AgentMove.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/AgentMove) |
| [CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation](https://arxiv.org/abs/2506.13599) | arXiv 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/CAMS.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/CAMS) |
| [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/abs/2506.02713) | ACL 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/PIGEON.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/PIGEON) |



### 5.4 Societal Intelligence

| Title                                                                                                                                   | Pub. & Date | Code / Project URL                                              |
| --------------------------------------------------------------------------------------------------------------------------------------- | ----------- | --------------------------------------------------------------- |
| [AgentSociety: Large-scale simulation of LLM-driven generative agents](https://arxiv.org/abs/2502.08691)                                | ACL 2025, COLM 2025 | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/AgentSociety.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/AgentSociety)                    |
| [GenSim: A General Social Simulation Platform with Large Language Model based Agents](https://aclanthology.org/2025.naacl-demo.15/)             | NAACL 2025  | [![Star](https://img.shields.io/github/stars/TangJiakai/GenSim.svg?style=social&label=Star)](https://github.com/TangJiakai/GenSim)   |
| [Simulating Human-like Daily Activities with Desire-driven Autonomy](https://arxiv.org/abs/2412.06435)             | ICLR 2025  | [![Star](https://img.shields.io/github/stars/zfw1226/D2A.svg?style=social&label=Star)](https://github.com/zfw1226/D2A)   |
| [EconAgent: Large language model-empowered agents for simulating macroeconomic activities](https://aclanthology.org/2024.acl-long.829/) | ACL 2024    | [![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/ACL24-EconAgent.svg?style=social&label=Star)](https://github.com/tsinghua-fib-lab/ACL24-EconAgent)  |
| [Agent-Pro: Learning to evolve via policy-level reflection and optimization](https://aclanthology.org/2024.acl-long.292/)                          | ACL 2024  | [![Star](https://img.shields.io/github/stars/zwq2018/Agent-Pro.svg?style=social&label=Star)](https://github.com/zwq2018/Agent-Pro)                     |
| [Exploring collaboration mechanisms for LLM agents: A social psychology view](https://aclanthology.org/2024.acl-long.782/)                         | ACL 2024  | [![Star](https://img.shields.io/github/stars/zjunlp/MachineSoM.svg?style=social&label=Star)](https://github.com/zjunlp/MachineSoM)                    |
| [Cooperate or Collapse: Emergence of sustainability behaviors in a society of LLM agents](https://neurips.cc/virtual/2024/poster/96895)             | NeurIPS 2024  | [![Star](https://img.shields.io/github/stars/giorgiopiatti/GovSim.svg?style=social&label=Star)](https://github.com/giorgiopiatti/GovSim)  |
| [SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series](https://iclr.cc/virtual/2024/poster/17662)             | ICLR 2024  | [![Star](https://img.shields.io/github/stars/chengjunyan1/SocioDojo.svg?style=social&label=Star)](https://github.com/chengjunyan1/SocioDojo)   |
| [SRAP-Agent: Simulating and optimizing scarce resource allocation policy with LLM-based agent](https://aclanthology.org/2024.findings-emnlp.15/)        | EMNLP 2024  | [![Star](https://img.shields.io/github/stars/Ji-Cather/SRAPAgent_Framework.svg?style=social&label=Star)](https://github.com/Ji-Cather/SRAPAgent_Framework)                     |
| [Generative agents: Interactive simulacra of human behavior](https://dl.acm.org/doi/10.1145/3586183.3606763)                            | UIST 2023   | [![Star](https://img.shields.io/github/stars/joonspk-research/generative_agents.svg?style=social&label=Star)](https://github.com/joonspk-research/generative_agents) |
| [SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users](https://arxiv.org/abs/2504.10157)             | arXiv 2025  | [![Star](https://img.shields.io/github/stars/FudanDISC/SocioVerse.svg?style=social&label=Star)](https://github.com/FudanDISC/SocioVerse)   |
| [YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models](https://arxiv.org/abs/2505.07581)             | arXiv 2025  | [![Star](https://img.shields.io/github/stars/RUC-GSAI/YuLan-OneSim.svg?style=social&label=Star)](https://github.com/RUC-GSAI/YuLan-OneSim)   |
| [OASIS: Open Agent Social Interaction Simulations with One Million Agents](https://arxiv.org/abs/2411.11581)             | arXiv 2024  | [![Star](https://img.shields.io/github/stars/camel-ai/oasis.svg?style=social&label=Star)](https://github.com/camel-ai/oasis)   |
| [Project Sid: Many-agent simulations toward AI civilization](https://arxiv.org/abs/2411.00114)                                          | arXiv 2024  | [![Star](https://img.shields.io/github/stars/altera-al/project-sid.svg?style=social&label=Star)](https://github.com/altera-al/project-sid)                    |
| [Network Formation and Dynamics Among Multi-LLMs](https://arxiv.org/abs/2402.10659)                                                     | arXiv 2024  | [![Star](https://img.shields.io/github/stars/papachristoumarios/llm-network-formation.svg?style=social&label=Star)](https://github.com/papachristoumarios/llm-network-formation)                    |
| [S3: Social-network Simulation System with Large Language Model-Empowered Agents](https://arxiv.org/abs/2307.14984)                     | arXiv 2023  | ‚Äî                    |
| [Exploring large language models for communication games: An empirical study on werewolf](https://arxiv.org/abs/2309.04658)             | arXiv 2023  | [![Star](https://img.shields.io/github/stars/xuyuzhuang11/Werewolf.svg?style=social&label=Star)](https://github.com/xuyuzhuang11/Werewolf)                     |
-->

## Broader Impacts of AI Scientists
| Title                                                        | Pub.&Date           | Code/Project URL                                             |
| ------------------------------------------------------------ | ------------------- | ------------------------------------------------------------ |
| [Large language models reflect human citation patterns with a heightened citation bias **(Citation Bias)**](https://arxiv.org/abs/2405.15739) | NAACL Findings 2025 | [![GitHub Repo stars](https://img.shields.io/github/stars/AndresAlgaba/LLM_citation_patterns?style=social)](https://github.com/AndresAlgaba/LLM_citation_patterns) |
| [How Deep Do Large Language Models Internalize Scientific Literature and Citation Practices **(Citation Practices)**](https://arxiv.org/abs/2504.02767) | arXiv 2025 | [![GitHub Repo stars](https://img.shields.io/github/stars/AndresAlgaba/LLMs_scientific_literature?style=social)](https://github.com/AndresAlgaba/LLMs_scientific_literature) |
| [Who Gets Recommended? Investigating Gender, Race, and Country Disparities in Paper Recommendations from Large Language Models **(LLM Recommendations Fairness)**](https://arxiv.org/abs/2406.12815) | arXiv 2024 | None |
| [AI‚ÄìAI bias: Large language models favor communications generated by large language models **(AI‚ÄìAI Bias)**](https://www.pnas.org/doi/10.1073/pnas.2415697122) | PNAS 2025 | [![GitHub Repo stars](https://img.shields.io/github/stars/lauritowal/ai-ai-bias?style=social)](https://github.com/lauritowal/ai-ai-bias) |
| [AI Expands Scientists' Impact but Contracts Science's Focus **(Impact vs. Focus)**](https://arxiv.org/abs/2412.07727) | arXiv 2024 |  [![GitHub Repo stars](https://img.shields.io/github/stars/tsinghua-fib-lab/AI-Impacts-Science?style=social)](https://github.com/tsinghua-fib-lab/AI-Impacts-Science) |
| [Quantifying the use and potential benefits of artificial intelligence in scientific research **(AI in Scientific Practice)**](https://www.nature.com/articles/s41562-024-02020-5) | Nat. Hum. Behav. 2024 | [Code](https://kellogg-cssi.github.io/ai4science/) |
| [Can GenAI Improve Academic Performance? Evidence from the Social and Behavioral Sciences **(GenAI & Education)**](https://arxiv.org/abs/2510.02408) | arXiv 2025 | None |
| [LLM4SR: A Survey on Large Language Models for Scientific Research **(LLM4SR)**](https://arxiv.org/abs/2501.04306) | arXiv 2025 | [![GitHub Repo stars](https://img.shields.io/github/stars/du-nlp-lab/LLM4SR?style=social)](https://github.com/du-nlp-lab/LLM4SR) |
| [The Role of AI in Facilitating Interdisciplinary Collaboration: Evidence from AlphaFold **(AlphaFold & Collaboration)**](https://www.arxiv.org/abs/2508.13234) | arXiv 2025 | None |
| [AI for Scientific Discovery is a Social Problem **(AI & Scientific Institutions)**](https://arxiv.org/abs/2509.06580) | arXiv 2025 | None |
| [Artificial intelligence in science ‚Äì Promises or perils for creativity? **(AI & Creativity)**](https://data.europa.eu/doi/10.2777/6693925) | EU Policy Report 2025 | None |
